{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198e0d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.13.0\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 429us/step - loss: 0.3020 - accuracy: 0.9122\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 430us/step - loss: 0.1451 - accuracy: 0.9563\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 435us/step - loss: 0.1098 - accuracy: 0.9658\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 436us/step - loss: 0.0886 - accuracy: 0.9732\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 437us/step - loss: 0.0765 - accuracy: 0.9759\n",
      "313/313 - 0s - loss: 0.0749 - accuracy: 0.9780 - 121ms/epoch - 385us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07487266510725021, 0.9779999852180481]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is just the tutorial from Colab with notes\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "# This is a database of handwritten digits that came from the tf website\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "# The above command initializes x and y train and test from the mnist database\n",
    "# Since the values are between 0 and 255, we scale between 0 and 1 by dividing the values by 255\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Here's the part where we build an actual machine learning model\n",
    "model = tf.keras.models.Sequential([\n",
    "    # It looks like the sequential model has a list of layers and a name for the model as possible arguemts\n",
    "    # It's very useful for stacking layers where each layer has one input tensor and one output tensor\n",
    "        # Tensor is the core framework, and all of the computations involve tensors\n",
    "        # tensors are vectors / matrices of n dimensions\n",
    "        # They can be from input data, or the output\n",
    "    # Layers are functions with a known math structure that can be reused, and have trainable variables\n",
    "    # tensorflow models are made out of layers\n",
    "    # the flatten, dense, and dropout functions below are layers\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    # flattens the input, but more importantly gives the input shape for the dense layer below\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "    #  Now the model will take as input arrays of shape (28, 28) and output arrays of shape (None, 128)?\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "    # Initializes BaseRandomLayer. Googled it, have no idea what this does\n",
    "  tf.keras.layers.Dense(10)\n",
    "    # Now we change the output array shape again\n",
    "])\n",
    "\n",
    "##predictions = model(x_train[:1]).numpy()\n",
    "# For each example, the model returns a vector of logits or log-odds scores, one for each class\n",
    "# A logit is the vector of raw, non normalized predictions that a classification model generates, which is ordinarily passed to normalize function\n",
    "# Predictions returns ten logits because it is 10 handwritten numbers that it's being trained on \n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "# This function converts the logits into probabilities for each class\n",
    "# Also .numpy() converts a tensor object into an numpuy.ndarray object, which means that the converted tensor will be now processed on the cpu\n",
    "# This vector gives that when it sees a number, it thinks that theres a 10% chance of it being any number\n",
    "##prob_predictions = tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "# print(prob_predictions)\n",
    "\n",
    "# this defines a los function for training\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# takes a vector of ground truth values and vector of logits and returns a scalar loss for each example\n",
    "# equal to neg log prob of true class; loss = 0 if model is sure of class\n",
    "# Untrained model gives 1/10 for each class, so -tf.math.log(1/10) ~= 2.3\n",
    "\n",
    "\n",
    "#print(loss_fn(y_train[:1], predictions).numpy())\n",
    "\n",
    "\n",
    "# Before training, we configure and compile the model using Keras Model.compile, set optimizer to adam, set loss to fn above, and specify metric to be evaluated\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "\n",
    "# We use the model.fit method to adjust the model parameters and minimize the loss\n",
    "# loss closer to 0 = better\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# The Model.evaluate method checks the model's performance, usually on a validation set or test set.\n",
    "# This is the test set that we imported along with the dataset\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "# Now this model is trained to 98% accuracy on this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32be0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  42 118 219 166 118 118   6\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 103 242 254 254 254 254 254  66\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  18 232 254 254 254 254 254 238\n",
      "   70   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 104 244 254 224 254 254 254\n",
      "  141   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 207 254 210 254 254 254\n",
      "   34   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  84 206 254 254 254 254\n",
      "   41   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  24 209 254 254 254\n",
      "  171   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  91 137 253 254 254 254\n",
      "  112   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  40 214 250 254 254 254 254 254\n",
      "   34   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  81 247 254 254 254 254 254 254\n",
      "  146   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 110 246 254 254 254 254 254\n",
      "  171   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  73  89  89  93 240 254\n",
      "  171   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 128 254\n",
      "  219  31   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7 254 254\n",
      "  214  28   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 138 254 254\n",
      "  116   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  19 177  90   0   0   0   0   0  25 240 254 254\n",
      "   34   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 164 254 215  63  36   0  51  89 206 254 254 139\n",
      "    8   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  57 197 254 254 222 180 241 254 254 253 213  11\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 140 105 254 254 254 254 254 254 236   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7 117 117 165 254 254 239  50   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "# The above command initializes x and y train and test from the mnist database\n",
    "# Since the values are between 0 and 255, we scale between 0 and 1 by dividing the values by 255\n",
    "print(x_train[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef218e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 176)\n",
      "Found 18 files belonging to 4 classes.\n",
      "Using 17 files for training.\n",
      "Found 18 files belonging to 4 classes.\n",
      "Using 1 files for validation.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"0028C_S.gif\")\n",
    "pix = np.array(im)\n",
    "print(pix.shape)\n",
    "#num = \n",
    "#lix = np.array([pix,num])\n",
    "#print(lix.shape)\n",
    "\n",
    "im_ht = 256\n",
    "im_wd = 256\n",
    "bat_sz = 2\n",
    "#training set\n",
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '/Users/zachderse//Desktop/cross_sectional_data/datasets/', \n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"categorical\", \n",
    "    color_mode = \"grayscale\", \n",
    "    batch_size = bat_sz,\n",
    "    seed = 123, \n",
    "    validation_split = .1, \n",
    "    image_size = (im_ht,im_wd),\n",
    "    subset = \"training\")\n",
    "\n",
    "#test set\n",
    "ds_valid = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '/Users/zachderse//Desktop/cross_sectional_data/datasets/', \n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"categorical\", \n",
    "    color_mode = \"grayscale\", \n",
    "    batch_size = bat_sz,\n",
    "    seed = 123, \n",
    "    validation_split = .1, \n",
    "    image_size = (im_ht,im_wd),\n",
    "    subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94cecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0.5\n",
      "none available\n",
      "none available\n",
      "none available\n",
      "none available\n",
      "none available\n",
      "0\n",
      "0\n",
      "none available\n",
      "0\n",
      "none available\n",
      "0.5\n",
      "0.5\n",
      "none available\n",
      "0\n",
      "0\n",
      "0\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "none available\n",
      "0\n",
      "none available\n",
      "1\n",
      "none available\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "none available\n",
      "none available\n",
      "0.5\n",
      "none available\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil  \n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "package_num = 1\n",
    "for i in range(1,42):\n",
    "    #print(\"This is the file number \",i, \": \", end=\"\")\n",
    "    try:\n",
    "        x = str(i).zfill(4)\n",
    "        file_name = (\"/Users/zachderse//Desktop/cross_sectional_data/disc1/OAS1_\"+x+\"_MR1/OAS1_\"+x+\"_MR1.txt\")\n",
    "        f = open(file_name)\n",
    "        textlines = f.readlines()\n",
    "        CDR_num = str(textlines[6][14:17])\n",
    "        CDR_num = CDR_num.strip()\n",
    "        \n",
    "        match CDR_num:\n",
    "            case \"0\":\n",
    "                print(\"0\")\n",
    "            case \"0.5\":\n",
    "                print(\"0.5\")\n",
    "            case \"1\":\n",
    "                print(\"1\")\n",
    "            case \"2\":\n",
    "                print(\"2\")\n",
    "  \n",
    "            case _ :\n",
    "                print(\"none available\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #orgfile = \"/Users/zachderse//Desktop/cross_sectional_data/disc1/OAS1_\"+x+\"_MR1/FSL_SEG/OAS1_\"+x+\"_MR1_mpr_n4_anon_111_t88_masked_gfc_fseg_tra_90.gif\"\n",
    "        #dest = \"/Users/zachderse//Desktop/cross_sectional_data/CDR_\"+str(CDR_num)+\"/\"+x+\"C_S.gif\"\n",
    "        #try:\n",
    "        #    shutil.copy(orgfile, dest)\n",
    "        #    print(\"copied successfully\")\n",
    "        #except shutil.SameFileError:\n",
    "        #    print(\"Source and destination represents the same file.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "887c661f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
